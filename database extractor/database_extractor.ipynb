{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "subject = 'IorNYll1lS' #! The ID of the subject on Back4app\n",
    "folder_path = 'database extractor/data'\n",
    "df_file = '20240413_165217_14ChannelSensorDatabase_mci017.db'\n",
    "db_file_path = f'{folder_path}/{df_file}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a connection to the SQLite database\n",
    "conn = sqlite3.connect(db_file_path)\n",
    "\n",
    "start_session = 156\n",
    "end_session = 195\n",
    "\n",
    "# Write your SQL query (replace 'your_table_name' with the actual table name)\n",
    "sql_query = f'SELECT * FROM USER_SESSION_TABLE WHERE SESSION_ID BETWEEN {start_session} AND {end_session}'\n",
    "\n",
    "# Use pandas to read data from the database into a DataFrame\n",
    "df = pd.read_sql(sql_query, conn)\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.0 seconds ---\n",
      "--- Right Query 15.796732902526855 seconds ---\n",
      "--- Left Query 29.92012619972229 seconds ---\n",
      "--- Complete in 35.07804322242737 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# Create a connection to the SQLite database\n",
    "conn = sqlite3.connect(db_file_path)\n",
    "\n",
    "# Write SQL queries for each table\n",
    "right_insole_query = f'SELECT * FROM RIGHT_INSOLE_RAW WHERE SESSION_ID BETWEEN {start_session} AND {end_session}'\n",
    "left_insole_query = f'SELECT * FROM LEFT_INSOLE_RAW WHERE SESSION_ID BETWEEN {start_session} AND {end_session}'\n",
    "\n",
    "# Use pandas to read data from each table into DataFrames\n",
    "right_insole_df = pd.read_sql(right_insole_query, conn)\n",
    "print(\"--- Right Query %s seconds ---\" % (time.time() - start_time))\n",
    "left_insole_df = pd.read_sql(left_insole_query, conn)\n",
    "print(\"--- Left Query %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# Close the database connection\n",
    "conn.close()\n",
    "\n",
    "# Group the DataFrames by SESSION_ID\n",
    "grouped_right_insole = right_insole_df.groupby('SESSION_ID').agg(list).reset_index()\n",
    "grouped_left_insole = left_insole_df.groupby('SESSION_ID').agg(list).reset_index()\n",
    "print(\"--- Complete in %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 0.0 seconds ---\n",
      "--- Read 0.004000663757324219 seconds ---\n",
      "--- Merged in 0.010000467300415039 seconds ---\n",
      "--- Filtered in 1.5944843292236328 seconds ---\n",
      "--- 13.795521259307861 seconds ---\n",
      "Session 156\n",
      "Session 157\n",
      "Session 158\n",
      "Session 159\n",
      "Session 160\n",
      "Session 161\n",
      "Session 162\n",
      "Session 163\n",
      "Session 164\n",
      "Session 165\n",
      "Session 166\n",
      "Session 167\n",
      "Session 168\n",
      "Session 169\n",
      "Session 170\n",
      "Session 171\n",
      "Session 172\n",
      "Session 173\n",
      "Session 174\n",
      "Session 175\n",
      "Session 176\n",
      "Session 177\n",
      "Session 178\n",
      "Session 179\n",
      "Session 180\n",
      "Session 181\n",
      "Session 182\n",
      "Session 183\n",
      "Session 184\n",
      "Session 185\n",
      "Session 186\n",
      "Session 187\n",
      "Session 188\n",
      "Session 189\n",
      "Session 190\n",
      "Session 191\n",
      "Session 192\n",
      "Session 193\n",
      "Session 194\n",
      "Session 195\n",
      "--- Completed in 25.942578077316284 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "# Create a connection to the SQLite database\n",
    "conn = sqlite3.connect(db_file_path)\n",
    "\n",
    "# Read USER_SESSION_TABLE to get STARTDATE for each SESSION_ID\n",
    "user_session_query = 'SELECT SESSION_ID, START_DATE FROM USER_SESSION_TABLE'\n",
    "user_session_df = pd.read_sql(user_session_query, conn)\n",
    "print(\"--- Read %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# Merge user_session_df with grouped_right_insole and grouped_left_insole\n",
    "merged_right_insole = pd.merge(grouped_right_insole, user_session_df, on='SESSION_ID', how='inner')\n",
    "merged_left_insole = pd.merge(grouped_left_insole, user_session_df, on='SESSION_ID', how='inner')\n",
    "\n",
    "\n",
    "print(\"--- Merged in %s seconds ---\" % (time.time() - start_time))\n",
    "# Close the database connection\n",
    "conn.close()\n",
    "\n",
    "\n",
    "# Root folder path\n",
    "root_folder_path = 'database extractor/data/mci006/'\n",
    "\n",
    "# Filter data for the specified SESSION_ID range\n",
    "filtered_right_insole = merged_right_insole[(merged_right_insole['SESSION_ID'] >= start_session) & (merged_right_insole['SESSION_ID'] <= end_session)]\n",
    "filtered_left_insole = merged_left_insole[(merged_left_insole['SESSION_ID'] >= start_session) & (merged_left_insole['SESSION_ID'] <= end_session)]\n",
    "\n",
    "print(\"--- Filtered in %s seconds ---\" % (time.time() - start_time))\n",
    "# Iterate through each row and save data as TXT in the specified folder\n",
    "for index, session in filtered_right_insole.iterrows():\n",
    "    session_id = session['SESSION_ID']\n",
    "    start_date = str(session['START_DATE'])\n",
    "    date_folder = start_date[:4] + '-' + start_date[4:6] + '-' + start_date[6:]\n",
    "    array_data = session.drop(['SESSION_ID', 'START_DATE', 'RAW_ID'])\n",
    "    session_df = pd.DataFrame(array_data).T\n",
    "    folder_path = os.path.join(root_folder_path, date_folder)\n",
    "    \n",
    "    # Convert each array to pandas Series\n",
    "    series_list = {col: pd.Series(data) for col, data in array_data.items()}\n",
    "    if len(series_list['READING_1']) < 2 / 0.05: #! data can be less than 2 seconds, which is not necessary\n",
    "        print(f'Session {session_id} less than 2 seconds, Skip')\n",
    "        print(\"-----------------------------------------------------------\")\n",
    "        continue\n",
    "    # Create a new DataFrame by concatenating the Series\n",
    "    session_df = pd.DataFrame(series_list)\n",
    "    \n",
    "    os.makedirs(folder_path, exist_ok=True)  # Create folder if it doesn't exist\n",
    "    filename = os.path.join(folder_path, f'S{session_id}_{start_date}_rawDataRight_{subject}.txt')\n",
    "    session_df.to_csv(filename, sep=',', header=False, index=False)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "for index, session in filtered_left_insole.iterrows():\n",
    "    session_id = session['SESSION_ID']\n",
    "    print(f'Session {session_id}')\n",
    "    start_date = str(session['START_DATE'])\n",
    "    date_folder = start_date[:4] + '-' + start_date[4:6] + '-' + start_date[6:]\n",
    "    array_data = session.drop(['SESSION_ID', 'START_DATE', 'RAW_ID'])\n",
    "    session_df = pd.DataFrame(array_data).T\n",
    "    folder_path = os.path.join(root_folder_path, date_folder)\n",
    "    \n",
    "    # Convert each array to pandas Series\n",
    "    series_list = {col: pd.Series(data) for col, data in array_data.items()}\n",
    "    if len(series_list['READING_1']) < 2 / 0.05: #! data can be less than 2 seconds, which is not necessary\n",
    "        print(f'Session {session_id} less than 2 seconds, Skip')\n",
    "        print(\"-----------------------------------------------------------\")\n",
    "        continue\n",
    "    # Create a new DataFrame by concatenating the Series\n",
    "    session_df = pd.DataFrame(series_list)\n",
    "    \n",
    "    os.makedirs(folder_path, exist_ok=True)  # Create folder if it doesn't exist\n",
    "    filename = os.path.join(folder_path, f'S{session_id}_{start_date}_rawDataLeft_{subject}.txt')\n",
    "    session_df.to_csv(filename, sep=',', header=False, index=False)\n",
    "    \n",
    "print(\"--- Completed in %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF29_PY39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
